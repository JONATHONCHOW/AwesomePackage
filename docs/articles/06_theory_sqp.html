<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="AwesomePackage">
<title>Models and Methods II: Fit PSD Model by SQP Algorithm • AwesomePackage</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.1.3/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.1.3/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Models and Methods II: Fit PSD Model by SQP Algorithm">
<meta property="og:description" content="AwesomePackage">
<meta property="og:image" content="https://jonathonchow.github.io/AwesomePackage/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">AwesomePackage</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.0.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/AwesomePackage.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/01_quick_start.html">AwesomePackage: Brief Introduction</a>
    <a class="dropdown-item" href="../articles/02_user_manual.html">AwesomePackage: User Manual 2.0.0</a>
    <a class="dropdown-item" href="../articles/03_report.html">AwesomePackage: A Valid R Package for Ancestry Inference</a>
    <a class="dropdown-item" href="../articles/04_presentation.html">AwesomePackage: A Valid R Package for Ancestry Inference (slide)</a>
    <a class="dropdown-item" href="../articles/05_theory_em.html">Models and Methods I: Fit PSD Model by EM Algorithm</a>
    <a class="dropdown-item" href="../articles/06_theory_sqp.html">Models and Methods II: Fit PSD Model by SQP Algorithm</a>
    <a class="dropdown-item" href="../articles/07_theory_vi.html">Models and Methods III: Fit PSD Model by VI Algorithm</a>
    <a class="dropdown-item" href="../articles/08_theory_svi.html">Models and Methods IV: Fit PSD Model by SVI Algorithm</a>
    <a class="dropdown-item" href="../articles/09_theory_model.html">Models and Methods V: Relationship between PSD Model and Other Models</a>
    <a class="dropdown-item" href="../articles/10_application_simu.html">Applications I: Simulated Data Sets</a>
    <a class="dropdown-item" href="../articles/11_application_tgp.html">Applications II: TGP Data Set</a>
    <a class="dropdown-item" href="../articles/12_application_hgdp.html">Applications III: HGDP Data Set</a>
    <a class="dropdown-item" href="../articles/13_note_fasttopics.html">Notes I: Package fastTopics</a>
    <a class="dropdown-item" href="../articles/14_note_faststructure.html">Notes II: Package fastSTRUCTURE</a>
    <a class="dropdown-item" href="../articles/15_note_terastructure.html">Notes III: Package TeraStucture</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/JONATHONCHOW/AwesomePackage/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Models and Methods II: Fit PSD Model by SQP Algorithm</h1>
                        <h4 data-toc-skip class="author">Jonathon
Chow</h4>
            
            <h4 data-toc-skip class="date">2022-10-04</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/JONATHONCHOW/AwesomePackage/blob/HEAD/vignettes/06_theory_sqp.Rmd" class="external-link"><code>vignettes/06_theory_sqp.Rmd</code></a></small>
      <div class="d-none name"><code>06_theory_sqp.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>We use the sequential quadratic programming algorithm (SQP) to fit
the PSD model <span class="citation">(Alexander, Novembre, and Lange
2009)</span>.</p>
</div>
<div class="section level2">
<h2 id="psd-model">PSD Model<a class="anchor" aria-label="anchor" href="#psd-model"></a>
</h2>
<p>  The typical data set consists of genotypes at a large number <span class="math inline">\(J\)</span> of single nucleotide polymorphisms
(SNPs) from a large number <span class="math inline">\(I\)</span> of
unrelated individuals. These individuals are drawn from an admixed
population with contributions from <span class="math inline">\(K\)</span> postulated ancestral populations.
Population <span class="math inline">\(k\)</span> contributes a fraction
<span class="math inline">\(p_{ik}\)</span> of individual <span class="math inline">\(i\)</span>’s genome. Note that <span class="math inline">\(\sum_{k=1}^Kp_{ik}=1\)</span>, and <span class="math inline">\(p_{ik}\geq 0\)</span>. Allele 1 at SNP <span class="math inline">\(j\)</span> has frequency <span class="math inline">\(f_{kj}\)</span> in population <span class="math inline">\(k\)</span>. Note that <span class="math inline">\(0\leq f_{kj}\leq 1\)</span>. As a matter of
convention, one can choose allele 1 to be the minor allele and the
alternative allele 2 to be the major allele. In our model, both the
<span class="math inline">\(p_{ik}\)</span> and the <span class="math inline">\(f_{kj}\)</span> are unknown. We are primarily
interested in estimating the <span class="math inline">\(p_{ik}\)</span>
to control for ancestry in an association study, but our approach also
yields estimates of the <span class="math inline">\(f_{kj}\)</span>.</p>
<p>  Let <span class="math inline">\((g_{ij}^1,g_{ij}^2)\)</span>
represents the genotype at marker <span class="math inline">\(j\)</span>
of person <span class="math inline">\(i\)</span>, where <span class="math inline">\(g_{ij}^a\)</span> represent the observed number of
copies of allele 1 at seat <span class="math inline">\(a\)</span>. Thus,
<span class="math inline">\((g_{ij}^1,g_{ij}^2)\)</span> equals <span class="math inline">\((1,1)\)</span>, <span class="math inline">\((1,0)\)</span>, <span class="math inline">\((0,1)\)</span>, or <span class="math inline">\((0,0)\)</span> accordingly, as <span class="math inline">\(i\)</span> has genotype 1/1, 1/2, 2/1, or 2/2 at
marker <span class="math inline">\(j\)</span>.</p>
<p>  Note that individuals are formed by the random union of gametes.
This produces the binomial proportions <span class="math display">\[P(g_{ij}^a=1)=\sum_{k=1}^Kp_{ik}f_{kj},\quad
P(g_{ij}^a=0)=\sum_{k=1}^Kp_{ik}(1-f_{kj}),\quad a=1,2.\]</span> Since
individuals <span class="math inline">\(i\)</span>, SNPs <span class="math inline">\(j\)</span>, and seats <span class="math inline">\(a\)</span> are considered independent, the
log-likelihood of the entire sample is <span class="math display">\[\mathcal{L}(G|P,F)=\sum_{i=1}^I\sum_{j=1}^J\sum_{a=1}^2\bigg\{g_{ij}^alog\Big[\sum_{k=1}^Kp_{ik}f_{kj}\Big]+(1-g_{ij}^a)log\Big[\sum_{k=1}^Kp_{ik}(1-f_{kj})\Big]\bigg\}\]</span>
up to an additive constant that does not enter into the maximization
problem. Let <span class="math inline">\(g_{ij}=g_{ij}^1+g_{ij}^2\)</span>. So the
log-likelihood can also be expressed as <span class="math display">\[\mathcal{L}(G|P,F)=\sum_{i=1}^I\sum_{j=1}^J\bigg\{g_{ij}log\Big[\sum_{k=1}^Kp_{ik}f_{kj}\Big]+(2-g_{ij})log\Big[\sum_{k=1}^Kp_{ik}(1-f_{kj})\Big]\bigg\}.\]</span>
The parameter matrices <span class="math inline">\(P=\{p_{ik}\}\)</span>
and <span class="math inline">\(F=\{f_{kj}\}\)</span> have dimensions
<span class="math inline">\(I\times K\)</span> and <span class="math inline">\(K\times J\)</span>, for a total of <span class="math inline">\(K(I+J)\)</span> parameters.</p>
<p>  Note that the log-likelihood is invariant under permutations of the
labels of the ancestral populations. Thus, the log-likelihood has at
least <span class="math inline">\(K!\)</span> equivalent global maxima.
In practice, this is a minor nuisance and does not affect the
convergence of well-behaved algorithms. The constraints <span class="math inline">\(0\leq f_{kj}\leq 1\)</span>, <span class="math inline">\(p_{ik}\geq 0\)</span>, and <span class="math inline">\(\sum_{k=1}^Kp_{ik}=1\)</span> are more significant
hindrances to contriving a good optimization algorithm.</p>
</div>
<div class="section level2">
<h2 id="quadratic-programming-problem-and-active-set-method">Quadratic Programming Problem and Active Set Method<a class="anchor" aria-label="anchor" href="#quadratic-programming-problem-and-active-set-method"></a>
</h2>
<div class="section level3">
<h3 id="general-optimization-theory">General optimization theory<a class="anchor" aria-label="anchor" href="#general-optimization-theory"></a>
</h3>
<p>  We consider an optimization problem in the standard form: <span class="math display">\[
\begin{split}
min &amp; \quad f_0(x) \\
s.t. &amp; \quad f_i(x)\leq 0,\quad i=1,\ldots,m \\
&amp; \quad h_i(x)=0,\quad i=1,\ldots,p,
\end{split}
\]</span> with variable <span class="math inline">\(x\in
\mathbb{R}^n\)</span>. We define the <span class="math inline">\(\mathcal{L}\)</span> as <span class="math display">\[\mathcal{L}(x,\lambda,\nu)=f_0(x)+\sum_{i=1}^m\lambda_if_i(x)+\sum_{i=1}^p\nu_ih_(x).\]</span>
We refer to <span class="math inline">\(\lambda_i\)</span> and <span class="math inline">\(\nu_i\)</span> as the .</p>
<p>  Let <span class="math inline">\(x^*\)</span> and <span class="math inline">\((\lambda^*, \nu^*)\)</span> be any primal and dual
optimal points with zero duality gap (strong duality), then it follows
the (KKT) conditions <span class="math display">\[
\begin{split}
f_i(x^*)\leq 0 &amp; ,\quad i=1,\ldots,m \\
h_i(x^*)=0 &amp; ,\quad i=1,\ldots,p \\
\lambda_i^*\geq 0 &amp; ,\quad i=1,\ldots,m \\
\lambda_i^*f_i(x^*)=0 &amp; ,\quad i=1,\ldots,m \\
\nabla f_0(x^*)+\sum_{i=0}^m\lambda_i^*\nabla
f_i(x^*)+\sum_{i=1}^p\nu_i^*\nabla h_i(x^*)&amp; .
\end{split}
\]</span></p>
<p>  When the primal problem is convex, the KKT conditions are also
sufficient for the points to be primal and dual optimal.</p>
<p>  A fundamental property of convex optimization problems is that any
locally optimal point is also (globally) optimal.</p>
</div>
<div class="section level3">
<h3 id="general-linear-constrained-quadratic-programming-problem">General linear constrained quadratic programming problem<a class="anchor" aria-label="anchor" href="#general-linear-constrained-quadratic-programming-problem"></a>
</h3>
<p>  We consider an quadratic programming problems with general linear
constraints in the standard form: <span class="math display">\[
\begin{aligned}
min &amp; \quad \frac{1}{2}x^TQx+c^Tx \\
s.t. &amp; \quad a_i^Tx-b_i\leq 0,\quad i\in\mathcal{I}=\{1,\ldots,m\}
\\
&amp; \quad a_i^Tx-b_i=0,\quad i\in\mathcal{E}=\{m+1,\ldots,m+l\},
\end{aligned}
\quad\quad(QP)
\]</span> where <span class="math inline">\(Q\)</span> is a positive
semidefinite matrix. So this is a convex optimization problem. We do not
consider the case where Q is an indefinite matrix, in fact this is an
NP-hard problem.</p>
<p>  We give the fundamental theorem for active sets. We consider the
following equality constrained quadratic programming problem <span class="math display">\[
\begin{aligned}
min &amp; \quad \frac{1}{2}x^TQx+c^Tx \\
s.t. &amp; \quad a_i^Tx-b_i\leq 0,\quad i\in\mathcal{I}(x^*) \\
&amp; \quad a_i^Tx-b_i=0,\quad i\in\mathcal{E},
\end{aligned}
\quad\quad(QP^*)
\]</span> where <span class="math inline">\(\mathcal{I}(x^*)=\{i|a_i^Tx^*=b_i,i\in\mathcal{I}\}\)</span>.</p>
<p>  If <span class="math inline">\(x^*\)</span> is an optimal solution
to <span class="math inline">\((QP)\)</span> then <span class="math inline">\(x^*\)</span> is also an optimal solution to <span class="math inline">\((QP^*)\)</span>. In fact, it is obtained by using
the equivalence between the optimal solution and the KKT condition of
convex optimization problems. This is geometrically intuitive, we use
the contour map and notice that the constraints are convex polyhedra, so
the optimal value can only be found at the boundary.</p>
<p>  Conversely, if <span class="math inline">\(x^*\)</span> is a
feasible solution of <span class="math inline">\((QP)\)</span> and an
optimal solution of <span class="math inline">\((QP^*)\)</span>, and its
corresponding <span class="math inline">\(\lambda_i^*\)</span> satisfies
<span class="math inline">\(\lambda_i^*\geq0\)</span>, for <span class="math inline">\(i\in\mathcal{I}(x^*)\)</span>, then <span class="math inline">\(x^*\)</span> is the optimal solution of <span class="math inline">\((QP)\)</span>. In fact, it is obtained by using
KKT conditions or complementary relaxation.</p>
<p>  Using the fundamental theorem for active sets, we transform the
inequality constrained quadratic programming problem into a finite
number of equality constrained quadratic programming problems, which is
guaranteed by . The quadratic programming problem with equality
constraints can be solved easily.</p>
</div>
<div class="section level3">
<h3 id="equality-constrained-quadratic-programming-problem">Equality constrained quadratic programming problem<a class="anchor" aria-label="anchor" href="#equality-constrained-quadratic-programming-problem"></a>
</h3>
<p>  We consider an quadratic programming problems with equality
constraints in the standard form: <span class="math display">\[
\begin{aligned}
min &amp; \quad \frac{1}{2}x^TQx+c^Tx \\
s.t. &amp; \quad Ax=b.
\end{aligned}
\]</span></p>
<p>There are many methods to solve equality constrained quadratic
programming problem <span class="citation">(Nocedal and Wright
2006)</span>, such as direct elimination method, Schur matrix
factorization method, generalized elimination method, iterative method
and so on.</p>
<p>  For the full rank matrix <span class="math inline">\(A\)</span> and
the positive definite matrix <span class="math inline">\(Q\)</span>,
these methods are all feasible. However, we may encounter the case where
<span class="math inline">\(Q\)</span> is positive semidefinite in
practice, then we solve this problem using Lagrange dual and QR
factorization. Note that for solving PSD model using SQP algorithm, the
constraints are compatible, and there are no negative infinity cases.
Hence, by the property of convex optimization, there must be a solution
to the problem. Since we are dealing with small matrices, there is no
need to use some sparse matrix tricks.</p>
<p>  We apply the KKT condition and get the equivalent statement <span class="math display">\[
\begin{bmatrix}
Q&amp;A^T\\A&amp;0
\end{bmatrix}
\begin{bmatrix}
x^*\\\lambda^*
\end{bmatrix}=
\begin{bmatrix}
-c\\b
\end{bmatrix}.
\]</span> Then, we solved this system of linear equations using QR
factorization.</p>
</div>
<div class="section level3">
<h3 id="inequality-constrained-quadratic-programming-problem">Inequality constrained quadratic programming problem<a class="anchor" aria-label="anchor" href="#inequality-constrained-quadratic-programming-problem"></a>
</h3>
<p>  For inequality constrained quadratic programming problem, there are
many methods to solve it, the most typical ones are active set method
<span class="citation">(Goldfarb and Idnani 1983)</span> and interior
point method <span class="citation">(Boyd and Vandenberghe 2004)</span>,
which converge quickly. We can also use alternating direction method of
multipliers <span class="citation">(Boyd et al. 2011)</span>, such as
Dykstra, which have a slower convergence rate. Specific algorithms can
be derived for special constraints or sparse matrices, such as OSQP and
LowRankQP.</p>
<p>  For our goal, we need to solve thousands of small quadratic
programming problems, so we do not require high convergence speed of the
algorithm and do not need to consider the case of sparse matrices. We
use the active set method, which works well for our problem. We also
tried the Dykstra algorithm, which performed less well. At the same
time, for this specific problem, compared with the general linear
constrained quadratic programming problem, there is a certain
convenience in algorithm implementation.</p>
<p>  Now, we derive the active set method.</p>
<p></p>
</div>
</div>
<div class="section level2">
<h2 id="sequential-quadratic-programming-algorithm">Sequential Quadratic Programming Algorithm<a class="anchor" aria-label="anchor" href="#sequential-quadratic-programming-algorithm"></a>
</h2>
<p>  One of the most effective methods for nonlinearly constrained
optimization generates steps by solving quadratic subproblems. This
sequential quadratic programming (SQP) approach can be used both in line
search and trust-region frameworks, and is appropriate for small or
large problems.</p>
<p>  First, consider the problem of equality constraints <span class="math display">\[
\begin{split}
min &amp; \quad f(x) \\
s.t. &amp; \quad c(x)=0,
\end{split}
\]</span> where<span class="math inline">\(c(x)=\big[c_1(x),\ldots,c_m(x)\big]^T\)</span>.</p>
<p>  The Lagrangian function for this problem is <span class="math inline">\(\mathcal{L}(x.\lambda)=f(x)-\lambda^Tc(x)\)</span>.
We use <span class="math inline">\(A(x)\)</span> to denote the Jacobian
matrix of the constraints, that <span class="math display">\[A(x)^T=[\nabla c_1(x),\ldots,\nabla
c_m(x)].\]</span> The first-order (KKT) conditions of the
equlity-constrained problem can be written as a system of <span class="math inline">\(n+m\)</span> equations in <span class="math inline">\(n+m\)</span> unknowns <span class="math inline">\(x\)</span> and <span class="math inline">\(\lambda\)</span>: <span class="math display">\[F(x,\lambda)=\begin{bmatrix}\nabla
f(x)-A(x)^T\lambda\\c(x)\end{bmatrix}=0.\]</span> Any solution <span class="math inline">\((x^*,\lambda^*)\)</span> of the
equality-constrained problem for which <span class="math inline">\(A(x^*)\)</span> has full rank satisfies the above
equations. We can solve the nonlinear equations by using Newton’s
method.</p>
<p>  The Jacobian of <span class="math inline">\(F(x,\lambda)\)</span>
is <span class="math display">\[F'(x,\lambda)=\begin{bmatrix}\nabla_{xx}^2\mathcal{L}(x,\lambda)&amp;-A(x)^T\\A(x)&amp;0\end{bmatrix}.\]</span>
The Newton step from the iterate <span class="math inline">\((x_k,\lambda_k)\)</span> is thus given by <span class="math display">\[\begin{bmatrix}x_{k+1}\\\lambda_{k+1}\end{bmatrix}=\begin{bmatrix}x_k\\\lambda_k\end{bmatrix}+\begin{bmatrix}p_k\\p_{\lambda}\end{bmatrix},\]</span>
where <span class="math inline">\(p_k\)</span> and <span class="math inline">\(p_{\lambda}\)</span> solve the Newton-KKT system
<span class="math display">\[\begin{bmatrix}\nabla_{xx}^2\mathcal{L}_k&amp;-A_k^T\\A_k&amp;0\end{bmatrix}\begin{bmatrix}p_k\\p_{\lambda}\end{bmatrix}=\begin{bmatrix}-\nabla
f_k+A_k^T\lambda_k\\-c_k\end{bmatrix}.\]</span> This Newton iteration is
well defined when the KKT matrix is nonsingular.</p>
<p>  There is an alternative way to view the iteration above. Suppose
that at the iterate <span class="math inline">\((x_k,\lambda_k)\)</span>
we model problem using the quadratic program <span class="math display">\[\begin{split}
\mathop{min}\limits_p &amp; \quad f_(x)_k+\nabla
f_k^Tp+\frac{1}{2}p^T\nabla_{xx}^2\mathcal{L}_kp \\
s.t. &amp; \quad A_kp+c_k=0.
\end{split}\]</span> If the original problem is true, this problem has a
unique solution <span class="math inline">\((p_k,l_k)\)</span> that
satisfies <span class="math display">\[\nabla_{xx}^2\mathcal{L}_kp_k+\nabla
f_k-A_k^Tl_k=0,\]</span> <span class="math display">\[A_kp_k+c_k=0.\]</span> The vectors <span class="math inline">\(p_k\)</span> and <span class="math inline">\(l_k\)</span> can be identified with the solution
of the Newton equations above as well. In fact, if we subtract <span class="math inline">\(A_k^T\lambda_k\)</span> from both sides of the
first equation in the Newton equations above, we obtain <span class="math display">\[\begin{bmatrix}\nabla_{xx}^2\mathcal{L}_k&amp;-A_k^T\\A_k&amp;0\end{bmatrix}\begin{bmatrix}p_k\\\lambda_{k+1}\end{bmatrix}=\begin{bmatrix}-\nabla
\lambda_k\\-c_k\end{bmatrix}.\]</span> Hence, by nonsingularity of the
coefficient matrix, we have that <span class="math inline">\(\lambda_{k+1}=l_k\)</span> and that <span class="math inline">\(p_k\)</span> solves the quadratic program and the
Newton equations.</p>
<p>  The new iterate <span class="math inline">\(x_{k+1},\lambda_{k+1}\)</span> can therefore be
defined either as the solution of the quadratic program or as the
iterate generated by Newton’s method applied to the optimality
conditions of the problem. Both viewpoints are useful. The Newton point
of view facilitates the analysis, whereas the SQP framework enables us
to derive practical algorithms and to extend the technique to the
inequality-constrained case.</p>
<p>  We now state the SQP method in its simplest form. First, choose an
initial pair <span class="math inline">\((x_0,\lambda_0)\)</span> and
set <span class="math inline">\(k=0\)</span>. Then, evaluate <span class="math inline">\(f_k\)</span>, <span class="math inline">\(\nabla
f_k\)</span>, <span class="math inline">\(\nabla_{xx}^2\mathcal{L}_k\)</span>, and <span class="math inline">\(A_k\)</span>. Next, solve the quadratic program to
obtain <span class="math inline">\(p_k\)</span>, <span class="math inline">\(l_k\)</span>. Finally, set <span class="math inline">\(x_{k+1}=x_k+p_k\)</span> and <span class="math inline">\(\lambda_{k+1}=\lambda_k\)</span>. Repeat the
process until a convergence test is satisfied.</p>
<p>  For the inequality-constrained case, we can use the active set
method.</p>
</div>
<div class="section level2">
<h2 id="fit-psd-model-by-sqp-algorithm">Fit PSD Model by SQP Algorithm<a class="anchor" aria-label="anchor" href="#fit-psd-model-by-sqp-algorithm"></a>
</h2>
<div class="section level3">
<h3 id="convexity-of-the-optimization-problem">Convexity of the optimization problem<a class="anchor" aria-label="anchor" href="#convexity-of-the-optimization-problem"></a>
</h3>
<p>  Our objective is to solve the following optimization problem <span class="math display">\[\begin{split}
\mathop{max}\limits_{P,F} &amp; \quad \mathcal{L}(G|P^{(t)},F^{(t)}) \\
s.t. &amp; \quad \sum_{k=1}^Kp_{ik}=1,\quad i=1,\ldots,I \\
&amp; \quad 0\leq p_{ik}\leq 1,\quad i=1,\ldots,I,\quad k=1,\ldots,K\\
&amp; \quad 0\leq f_{kj}\leq 1,\quad j=1,\ldots,J,\quad k=1,\ldots,K.
\end{split}\]</span></p>
<p>  We can easily translate this optimization problem into a standard
form. This optimization problem is convex in P for F fixed and in F for
P fixed. In fact, by the definition of a convex optimization problem, we
only need to show that the objective function is convex, the inequality
constraint function is convex, and the equality constraint function is
affine. Note that these three conditions naturally lead to the fact that
the feasible set is convex. The last two conditions are clearly
satisfied. We just have to prove the fact that the negative
log-likelihood <span class="math inline">\(-\mathcal{L}(G|P,F)\)</span>
is convex in P for F fixed and in F for P fixed.</p>
<p>  We exploit two properties of convex functions. First, a nonnegative
weighted sum of convex functions <span class="math inline">\(f=\omega_1f_1+\cdots+\omega_mf_m\)</span> is
convex. Second, the convexity of the composite affine map is preserved.
Specifically, suppose <span class="math inline">\(f:\mathbb{R}^n\rightarrow\mathbb{R}\)</span>,
<span class="math inline">\(A\in\mathbb{R}^{n\times m}\)</span>, and
<span class="math inline">\(b\in\mathbb{R}^n\)</span>. Define <span class="math inline">\(g:\mathbb{R}^m\rightarrow\mathbb{R}\)</span> by
<span class="math inline">\(g(x)=f(Ax+b)\)</span>, wuth <span class="math inline">\(\textbf{dom}(g)=\{x|Ax+b\in\textbf{dom}(f)\}\)</span>.
Then, if <span class="math inline">\(f\)</span> is convex, so is <span class="math inline">\(g\)</span>.</p>
<p>  Using the first property, we simply state that <span class="math inline">\(-log\Big[\sum_{k=1}^Kp_{ik}f_{kj}\Big]\)</span> is
convex, <span class="math inline">\(-log\Big[\sum_{k=1}^Kp_{ik}(1-f_{kj})\Big]\)</span>
is convex can be proved similarly. Using the second property, and
concavity of the <span class="math inline">\(log\)</span> function, this
is obvious.</p>
<p>  Convexity makes block iteration amenable to convex optimization
techniques.</p>
</div>
<div class="section level3">
<h3 id="derivation-of-the-algorithm">Derivation of the algorithm<a class="anchor" aria-label="anchor" href="#derivation-of-the-algorithm"></a>
</h3>
<p>  We fix <span class="math inline">\(P\)</span> to optimize <span class="math inline">\(F\)</span>, then fix <span class="math inline">\(F\)</span> to optimize <span class="math inline">\(P\)</span>, and so on. We use the method at each
step, that is, the second-order Taylor expansion approximation <span class="math display">\[f(x)=f(x^{(t)})+\nabla
f(x^{(t)})(x-x^{(t)})+\frac{1}{2}(x-x^{(t)})^T\nabla^2f(x^{(t)})(x-x^{(t)})\]</span>
is used to transform the original problem into a constrained quadratic
programming problem. Let<span class="math inline">\(\Delta
x^{(t)}=x-x^{(t)}\)</span>.</p>
<p>  Update <span class="math inline">\(P\)</span>. We calculate the
first and second partial derivatives (Hessian matrix) of <span class="math inline">\(P\)</span> under the condition that <span class="math inline">\(F\)</span> is fixed <span class="math display">\[\frac{\partial\mathcal{L}}{\partial
p_{ik}}=\sum_{j=1}^J\bigg[\frac{g_{ij}f_{kj}}{\sum_{k=1}^Kp_{ik}f_{kj}}+\frac{(2-g_{ij})(1-f_{kj})}{\sum_{k=1}^Kp_{ik}(1-f_{kj})}\bigg],\]</span>
<span class="math display">\[\frac{\partial^2\mathcal{L}}{\partial
p_{ik}\partial
p_{il}}=-\sum_{j=1}^J\bigg[\frac{g_{ij}f_{kj}f_{lj}}{(\sum_{k=1}^Kp_{ik}f_{kj})^2}+\frac{(2-g_{ij})(1-f_{kj})(1-f_{lj})}{(\sum_{k=1}^Kp_{ik}(1-f_{kj}))^2}\bigg].\]</span>
We then solve the quadratic programming problem <span class="math display">\[\begin{split}
\mathop{min}\limits_{\Delta P_i} &amp; \quad \frac{1}{2}(\Delta
P_i)^T\Big[-\nabla^2_{P_i}\mathcal{L}(G|P^{(t)},F^{(t)})\Big]\Delta
P_i-\Big[\nabla_{P_i}\mathcal{L}(G|P^{(t)},F^{(t)})\Big]^T\Delta
P_i-\mathcal{L}(G|P^{(t)},F^{(t)}) \\
s.t. &amp; \quad 1^T\Delta P_i=0 \\
&amp; \quad 1-P_i^{(t)}\geq\Delta P_i\geq-P_i^{(t)}
\end{split}\]</span> using the active set method. Thus, <span class="math inline">\(P_i^{(t+1)}=P_i^{(t)}+\Delta P_i\)</span>.</p>
<p>  Update <span class="math inline">\(F\)</span>. We calculate the
first and second partial derivatives (Hessian matrix) of <span class="math inline">\(F\)</span> under the condition that <span class="math inline">\(P\)</span> is fixed <span class="math display">\[\frac{\partial\mathcal{L}}{\partial
f_{kj}}=\sum_{i=1}^I\bigg[\frac{g_{ij}p_{ik}}{\sum_{k=1}^Kp_{ik}f_{kj}}-\frac{(2-g_{ij})p_{ik}}{\sum_{k=1}^Kp_{ik}(1-f_{kj})}\bigg],\]</span>
<span class="math display">\[\frac{\partial^2\mathcal{L}}{\partial
f_{kj}\partial
f_{lj}}=-\sum_{i=1}^I\bigg[\frac{g_{ij}p_{ik}p_{il}}{(\sum_{k=1}^Kp_{ik}f_{kj})^2}+\frac{(2-g_{ij})p_{ik}p_{il}}{(\sum_{k=1}^Kp_{ik}(1-f_{kj}))^2}\bigg].\]</span>
We then solve the quadratic programming problem <span class="math display">\[\begin{split}
\mathop{min}\limits_{\Delta F_j} &amp; \quad \frac{1}{2}(\Delta
F_j)^T\Big[-\nabla^2_{F_j}\mathcal{L}(G|P^{(t)},F^{(t)})\Big]\Delta
F_j-\Big[\nabla_{F_j}\mathcal{L}(G|P^{(t)},F^{(t)})\Big]^T\Delta
F_j-\mathcal{L}(G|P^{(t)},F^{(t)}) \\
s.t. &amp; \quad 1-F_j^{(t)}\geq\Delta F_j\geq-F_j^{(t)}
\end{split}\]</span> using the active set method. Thus, <span class="math inline">\(F_j^{(t+1)}=F_j^{(t)}+\Delta F_j\)</span>.</p>
<p>  In conclusion, We update <span class="math inline">\(P\)</span> and
<span class="math inline">\(F\)</span> block by block alternately, until
the log-likelihood of incomplete data <span class="math inline">\(\mathcal{L}(G|P,F)\)</span> converges. This is the
SQP algorithm for PSD model.</p>
</div>
</div>
<div class="section level2">
<h2 id="acceleration">Acceleration<a class="anchor" aria-label="anchor" href="#acceleration"></a>
</h2>
<p>  We can speed up the SQP algorithm in three ways. First, We write
the core parameter update part in C++. Second, We can choose different
methods to solve quadratic programming problems and more specifically
solve large-scale systems of small linear equations. Third, we can use
quasi Newtonian method to accelerate the algorithm, The effect of this
method is remarkable.</p>
</div>
<div class="section level2">
<h2 id="algorithm-implementation">Algorithm Implementation<a class="anchor" aria-label="anchor" href="#algorithm-implementation"></a>
</h2>
<p>  Select a suitable QR decomposition of the Eigen library <span class="citation">(Guennebaud et al. 2022)</span>, The speed and accuracy
are compared in the following table.</p>
<table class="table">
<colgroup>
<col width="23%">
<col width="17%">
<col width="23%">
<col width="23%">
<col width="12%">
</colgroup>
<thead><tr class="header">
<th align="left">Method</th>
<th align="left">Requirements</th>
<th align="left">Small.Scale.Speed</th>
<th align="left">Large.scale.Speed</th>
<th align="left">Accuracy</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"><code>partial PivLu()</code></td>
<td align="left">Invertible</td>
<td align="left"><code>++</code></td>
<td align="left"><code>++</code></td>
<td align="left"><code>+</code></td>
</tr>
<tr class="even">
<td align="left"><code>full PivLu()</code></td>
<td align="left">None</td>
<td align="left"><code>-</code></td>
<td align="left"><code>--</code></td>
<td align="left"><code>+++</code></td>
</tr>
<tr class="odd">
<td align="left"><code>householderQr()</code></td>
<td align="left">None</td>
<td align="left"><code>++</code></td>
<td align="left"><code>++</code></td>
<td align="left"><code>+</code></td>
</tr>
<tr class="even">
<td align="left"><code>colPiv HouseholderQr()</code></td>
<td align="left">None</td>
<td align="left"><code>+</code></td>
<td align="left"><code>-</code></td>
<td align="left"><code>+++</code></td>
</tr>
<tr class="odd">
<td align="left"><code>fullPiv HouseholderQr()</code></td>
<td align="left">None</td>
<td align="left"><code>-</code></td>
<td align="left"><code>--</code></td>
<td align="left"><code>+++</code></td>
</tr>
<tr class="even">
<td align="left"><code>complete Orthogonal Decomposition()</code></td>
<td align="left">None</td>
<td align="left"><code>+</code></td>
<td align="left"><code>-</code></td>
<td align="left"><code>+++</code></td>
</tr>
<tr class="odd">
<td align="left"><code>llt()</code></td>
<td align="left">Positive definite</td>
<td align="left"><code>+++</code></td>
<td align="left"><code>+++</code></td>
<td align="left"><code>+</code></td>
</tr>
<tr class="even">
<td align="left"><code>ldlt()</code></td>
<td align="left">Positive or negative semidefinite</td>
<td align="left"><code>+++</code></td>
<td align="left"><code>+</code></td>
<td align="left"><code>++</code></td>
</tr>
<tr class="odd">
<td align="left"><code>bdcSvd()</code></td>
<td align="left">None</td>
<td align="left"><code>-</code></td>
<td align="left"><code>-</code></td>
<td align="left"><code>+++</code></td>
</tr>
<tr class="even">
<td align="left"><code>jacobiSvd()</code></td>
<td align="left">None</td>
<td align="left"><code>-</code></td>
<td align="left"><code>---</code></td>
<td align="left"><code>+++</code></td>
</tr>
</tbody>
</table>
<p>  We present the implementation of SQP algorithm in R package
AwesomePackage. You can fit the PSD model using the SQP algorithm using
the function <code>psd_fit_sqp</code>. At the same time, you can use
<code>plot_loss</code> to see changes in log-likelihood and
<code>plot_structure</code> to plot structure.</p>
<p>  Here is an example.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/JONATHONCHOW/AwesomePackage" class="external-link">AwesomePackage</a></span><span class="op">)</span></span>
<span><span class="va">G</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">1</span>, <span class="fl">0</span>,<span class="fl">2</span>,<span class="fl">1</span>, <span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">1</span>, <span class="fl">0</span>,<span class="fl">1</span>,<span class="fl">0</span>, <span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span>, <span class="fl">3</span>, <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/psd_fit_sqp.html">psd_fit_sqp</a></span><span class="op">(</span><span class="va">G</span>, <span class="fl">2</span>, <span class="fl">1e-5</span>, <span class="fl">50</span>, <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">result</span></span></code></pre></div>
<pre><code><span><span class="co">## $P</span></span>
<span><span class="co">##             [,1]        [,2]</span></span>
<span><span class="co">## [1,] 0.999999999 0.000000001</span></span>
<span><span class="co">## [2,] 0.000000001 0.999999999</span></span>
<span><span class="co">## [3,] 0.689179272 0.310820728</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $F</span></span>
<span><span class="co">##             [,1]  [,2]        [,3]        [,4]        [,5]</span></span>
<span><span class="co">## [1,] 0.276578102 1e-09 0.573747398 0.000000001 0.311700456</span></span>
<span><span class="co">## [2,] 0.000000001 1e+00 0.000000001 0.413493316 0.000000001</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $Loss</span></span>
<span><span class="co">## [1] -0.7211319 -0.7074246 -0.7074246</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $Iterations</span></span>
<span><span class="co">## [1] 30</span></span></code></pre>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">L</span> <span class="op">&lt;-</span> <span class="va">result</span><span class="op">$</span><span class="va">Loss</span></span>
<span><span class="fu"><a href="../reference/plot_loss.html">plot_loss</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">L</span><span class="op">)</span>, <span class="st">"sqp"</span>, <span class="fl">10</span><span class="op">)</span></span></code></pre></div>
<p><img src="06_theory_sqp_files/figure-html/unnamed-chunk-2-1.png" width="700"></p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">P</span> <span class="op">&lt;-</span> <span class="va">result</span><span class="op">$</span><span class="va">P</span></span>
<span><span class="fu"><a href="../reference/plot_structure.html">plot_structure</a></span><span class="op">(</span><span class="va">P</span><span class="op">)</span></span></code></pre></div>
<p><img src="06_theory_sqp_files/figure-html/unnamed-chunk-2-2.png" width="700"></p>
<p>  See <a href="https://jonathonchow.github.io/AwesomePackage/">AwesomePackage</a>
for details.</p>
</div>
<div class="section level2">
<h2 class="unnumbered" id="literature-cited">Literature Cited<a class="anchor" aria-label="anchor" href="#literature-cited"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-alexander2009fast" class="csl-entry">
Alexander, David H, John Novembre, and Kenneth Lange. 2009. <span>“Fast
Model-Based Estimation of Ancestry in Unrelated Individuals.”</span>
<em>Genome Research</em> 19 (9): 1655–64.
</div>
<div id="ref-boyd2011distributed" class="csl-entry">
Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein,
et al. 2011. <span>“Distributed Optimization and Statistical Learning
via the Alternating Direction Method of Multipliers.”</span>
<em>Foundations and Trends<span></span> in Machine Learning</em> 3 (1):
1–122.
</div>
<div id="ref-boyd2004convex" class="csl-entry">
Boyd, Stephen, and Lieven Vandenberghe. 2004. <em>Convex
Optimization</em>. Cambridge University Press.
</div>
<div id="ref-goldfarb1983numerically" class="csl-entry">
Goldfarb, Donald, and Ashok Idnani. 1983. <span>“A Numerically Stable
Dual Method for Solving Strictly Convex Quadratic Programs.”</span>
<em>Mathematical Programming</em> 27 (1): 1–33.
</div>
<div id="ref-guennebaud2022api" class="csl-entry">
Guennebaud, Gaël et al. 2022. <span>“The API Documentation for
Eigen3.”</span> 2022. <a href="https://eigen.tuxfamily.org/dox/" class="external-link">https://eigen.tuxfamily.org/dox/</a>.
</div>
<div id="ref-nocedal2006numerical" class="csl-entry">
Nocedal, Jorge, and Stephen J Wright. 2006. <em>Numerical
Optimization</em>. Springer.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Jonathon Chow.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
