<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="AwesomePackage">
<title>Models and Methods III: Fit PSD Model by VI Algorithm • AwesomePackage</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.1.3/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.1.3/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Models and Methods III: Fit PSD Model by VI Algorithm">
<meta property="og:description" content="AwesomePackage">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">AwesomePackage</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.8.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/AwesomePackage.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/01_quick_start.html">AwesomePackage: Quick Start</a>
    <a class="dropdown-item" href="../articles/02_user_manual.html">AwesomePackage: User Manual 1.4.0</a>
    <a class="dropdown-item" href="../articles/03_report.html">AwesomePackage: A Valid R Package for Ancestry Inference</a>
    <a class="dropdown-item" href="../articles/04_presentation.html">AwesomePackage: A Valid R Package for Ancestry Inference (slide)</a>
    <a class="dropdown-item" href="../articles/05_theory_em.html">Models and Methods I: Fit PSD Model by EM Algorithm</a>
    <a class="dropdown-item" href="../articles/06_theory_sqp.html">Models and Methods II: Fit PSD Model by SQP Algorithm</a>
    <a class="dropdown-item" href="../articles/07_theory_vi.html">Models and Methods III: Fit PSD Model by VI Algorithm</a>
    <a class="dropdown-item" href="../articles/08_theory_svi.html">Models and Methods IV: Fit PSD Model by SVI Algorithm</a>
    <a class="dropdown-item" href="../articles/09_theory_model.html">Models and Methods V: Relationship between PSD Model and Other Models</a>
    <a class="dropdown-item" href="../articles/10_application_simu.html">Applications I: Simulated Data Sets</a>
    <a class="dropdown-item" href="../articles/11_application_tgp.html">Applications II: TGP Data Set</a>
    <a class="dropdown-item" href="../articles/12_application_hgdp.html">Applications III: HGDP Data Set</a>
    <a class="dropdown-item" href="../articles/13_note_fasttopics.html">Notes I: Package fastTopics</a>
    <a class="dropdown-item" href="../articles/14_note_faststructure.html">Notes II: Package fastSTRUCTURE</a>
    <a class="dropdown-item" href="../articles/15_note_terastructure.html">Notes III: Package TeraStucture</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/JONATHONCHOW/AwesomePackage/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Models and Methods III: Fit PSD Model by VI Algorithm</h1>
                        <h4 data-toc-skip class="author">Jonathon Chow</h4>
            
            <h4 data-toc-skip class="date">2022-10-01</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/JONATHONCHOW/AwesomePackage/blob/HEAD/vignettes/07_theory_vi.Rmd" class="external-link"><code>vignettes/07_theory_vi.Rmd</code></a></small>
      <div class="d-none name"><code>07_theory_vi.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>  We use the variational inference algorithm (VI) to fit the PSD model <span class="citation">(Raj, Stephens, and Pritchard 2014)</span>.</p>
</div>
<div class="section level2">
<h2 id="psd-model">PSD Model<a class="anchor" aria-label="anchor" href="#psd-model"></a>
</h2>
<p>  Suppose we have <span class="math inline">\(I\)</span> diploid individuals genotyped at <span class="math inline">\(J\)</span> biallelic loci. Let <span class="math inline">\((g_{ij}^1,g_{ij}^2)\)</span> represents the genotype at marker <span class="math inline">\(j\)</span> of person <span class="math inline">\(i\)</span>, where <span class="math inline">\(g_{ij}^a\)</span> represent the observed number of copies of allele 1 at seat <span class="math inline">\(a\)</span>. Thus, <span class="math inline">\((g_{ij}^1,g_{ij}^2)\)</span> equals <span class="math inline">\((1,1)\)</span>, <span class="math inline">\((1,0)\)</span>, <span class="math inline">\((0,1)\)</span>, or <span class="math inline">\((0,0)\)</span> accordingly, as <span class="math inline">\(i\)</span> has genotype 1/1, 1/2, 2/1, or 2/2 at marker <span class="math inline">\(j\)</span>. Let <span class="math inline">\(g_{ij}=g_{ij}^1+g_{ij}^2\)</span>. These individuals are drawn from an admixed population with contributions from <span class="math inline">\(K\)</span> postulated ancestral populations. Population <span class="math inline">\(k\)</span> contributes a fraction <span class="math inline">\(p_{ik}\)</span> of individual <span class="math inline">\(i\)</span>’s genome. Note that <span class="math inline">\(\sum_{k=1}^Kp_{ik}=1\)</span>, and <span class="math inline">\(p_{ik}\geq 0\)</span>. Allele 1 at SNP <span class="math inline">\(j\)</span> has frequency <span class="math inline">\(f_{kj}\)</span> in population <span class="math inline">\(k\)</span>. Note that <span class="math inline">\(0\leq f_{kj}\leq 1\)</span>. Similar to the EM algorithm, we consider <span class="math inline">\((z_{ij}^1,z_{ij}^2)\)</span>, where <span class="math inline">\(z_{ij}^a\)</span> is an element of the set <span class="math inline">\(\{1,\ldots,K\}\)</span>, denotes the population from which the genes of individual <span class="math inline">\(i\)</span> of marker <span class="math inline">\(j\)</span> at position <span class="math inline">\(a\)</span> really come. Let <span class="math inline">\(z_{ijk}^a=\textbf{1}(z_{ij}^a=k)\)</span>, obviously, <span class="math inline">\(z_{ijk}^a\in\{0,1\}\)</span>, and <span class="math inline">\(\sum_{k=1}^Kz_{ijk}^a=1\)</span>.</p>
<p>  Different from EM and SQP algorithms, variational inference does not optimize the maximum likelihood <span class="math inline">\(\mathcal{L}(G|P,F)\)</span>, but the posterior <span class="math inline">\(P(Z,P,F|G)\)</span>, which also reflects the difference between the frequency school and the Bayesian school. To be specific, under the previous assumptions, the observed variable is <span class="math inline">\(G\)</span>, the unobserved variables include latent variable <span class="math inline">\(Z\)</span> and parameters <span class="math inline">\(P\)</span>, <span class="math inline">\(F\)</span>. We take all unobserved variables together as variational objects, which are all involved in the estimation. At the same time, we take the model parameter <span class="math inline">\(K\)</span>, which is not involved in the estimation, as the hyperparameter.</p>
</div>
<div class="section level2">
<h2 id="variational-inference-algorithm">Variational Inference Algorithm<a class="anchor" aria-label="anchor" href="#variational-inference-algorithm"></a>
</h2>
<p>  The goal of Bayesian analysis is to estimate the posterior. In variational inference, we use a family of densities over the latent variables <span class="math inline">\(Q(z)\)</span>, parameterized by free  to approximate the true posterior <span class="math inline">\(P(z|x)\)</span> <span class="citation">(Blei, Kucukelbir, and McAuliffe 2017)</span>. In variational inference, we only examine latent variables without considering model parameters. We can transform it into an optimization problem, namely the minimization problem of KL divergence <span class="math display">\[\hat{Q}(z)=\mathop{argmin}\limits_{Q(z)}KL(Q(z)\|P(z|x)).\]</span> In addition, KL divergence generally cannot be directly solved. Note that <span class="math display">\[log P(x)=ELBO(Q(z)) + KL(Q(z)\|P(z|x)),\]</span> we can transform the problem into the maximization problem of ELBO <span class="math display">\[\hat{Q}(z)=\mathop{argmax}\limits_{Q(z)}ELBO(Q(z)).\]</span></p>
<p>  we focus on the , where the latent variables are mutually independent and each governed by a distinct factor in the variational density. A generic member of the mean-field variational family is <span class="math inline">\(Q(z)=\prod_{j=1}^{m}Q_j(z_j)\)</span>. Emphasize that <span class="math inline">\(Q(z)\)</span> here should be parameterized by free  <a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;For computational convenience, we usually choose parametric distributions that are conjugate to the distributions in the likelihood function.&lt;/p&gt;"><sup>1</sup></a>.</p>
<p>  Now we derive coordinate ascent variational inference (CAVI). We compute ELBO <span class="math display">\[ELBO(Q(z))=\int_{z}Q(z)log P(x,z)dz-\int_{z}Q(z)log Q(z)dz.\]</span> Note that the two terms of ELBO reflect the balance between likelihood and prior. Consider only <span class="math inline">\(Q_j\)</span> and fix the other <span class="math inline">\(Q_i\)</span>, <span class="math inline">\(i \neq j\)</span>, we have <span class="math display">\[\begin{split}
&amp;\int_{z}Q(z)log P(x,z)dz \\
=&amp; \int_{z}\prod_{i=1}^mQ_i(z_i)logP(x,z)dz \\
=&amp; \int_{z_j}Q_j(z_j)\Big[\int_{z_{-j}}\prod\limits_{i\neq j}Q_i(z_i)logP(x,z)dz_{-j}\Big]dz_j \\
=&amp; \int_{z_j}Q_j(z_j)\mathbb{E}_{\prod\limits_{i\neq j}Q_j(z_j)}\Big[log P(x,z)\Big]dz_j \\
:=&amp; \int_{z_j}Q_j(z_j)log \hat{P}(x,z_j)dz_j,
\end{split}\]</span> where <span class="math inline">\(z_{-j}\)</span> means to traverse all elements except <span class="math inline">\(j\)</span>. For the second term, <span class="math display">\[\begin{split}
&amp; \int_{z}Q(z)log Q(z)dz\\
=&amp; \int_{z}\prod_{i=1}^mQ_i(z_i)\sum_{i=1}^mlogQ_i(z_i)dz \\
=&amp; \sum_{i=1}^m\int_{z_i}Q_i(z_i)logQ_i(z_i)\Big[\int_{z_{-i}}\prod_{k\neq i}Q_k(z_k)dz_{-i}\Big]dz_i \\
=&amp; \sum_{i=1}^m\int_{z_i}Q_i(z_i)logQ_i(z_i)dz_i \\
=&amp; \int_{z_j}Q_j(z_j)logQ_j(z_j)dz_j+Const.
\end{split}\]</span> Thus, ELBO can be denoted as <span class="math display">\[ELBO(Q(z)) = \int_{z_j}Q_j(z_j)log\frac{\hat{P}(x,z_j)}{Q_j(z_j)}dz_j+Const = -KL(Q_j(z_j)\|\hat{P}(x,z_j))+Const \leq Const,\]</span> the equality holds if and only if <span class="math inline">\(\hat{Q}_j(z_j)=\hat{P}(x,z_j)\)</span>, this is the coordinate update formula of CAVI. We can use this formula (equivalent to ELBO taking the partial derivative of 0 with respect to <span class="math inline">\(Q(z_j)\)</span>) to obtain iterations of the variational parameters, and the above derivation procedure guarantees that ELBO will eventually converge to (local) minima.</p>
<p>  In conclusion, we first select an appropriate parameterized variational family, and then we use the coordinate ascent formula to update the variational parameters iteratively until ELBO converges. This is the VI algorithm.</p>
</div>
<div class="section level2">
<h2 id="fit-psd-model-by-vi-algorithm">Fit PSD Model by VI Algorithm<a class="anchor" aria-label="anchor" href="#fit-psd-model-by-vi-algorithm"></a>
</h2>
<div class="section level3">
<h3 id="the-variational-family">The variational family<a class="anchor" aria-label="anchor" href="#the-variational-family"></a>
</h3>
<p>  The choice of the variational family is restricted only by the tractability of computing expectations with respect to the variational distributions; here, we choose parametric distributions that are conjugate to the distributions in the likelihood function. Note that the likelihood function is <span class="math display">\[\begin{split}
&amp;P(G,Z,P,F|K)\\
=&amp;P(G|Z,P,F,K)P(Z|P,F,K)P(P,F|K)\\
=&amp;\prod_{i=1}^I\prod_{j=1}^J\prod_{k=1}^K\prod_{a=1}^2Binomial\Big(g_{ij}^a\Big|1,f_{kj}\Big)^{z_{ijk}^a}\cdot\prod_{i=1}^I\prod_{j=1}^J\prod_{a=1}^2Multinomial\Big(\big(z_{ij1}^a,\ldots,z_{ijK}^a\big)\Big|1,\big(p_{i1},\ldots,p_{iK}\big)\Big)\cdot P(P,F|K)\\
=&amp;\prod_{i=1}^I\prod_{j=1}^J\prod_{a=1}^2\bigg[Multinomial\Big(\big(z_{ij1}^a,\ldots,z_{ijK}^a\big)\Big|1,\big(f_{1j}^{g_{ij}^a}(1-f_{1j})^{1-{g_{ij}^a}}p_{i1},\ldots,f_{Kj}^{g_{ij}^a}(1-f_{Kj})^{1-{g_{ij}^a}}p_{iK}\big)\Big)\bigg]\cdot P(P,F|K).
\end{split}\]</span> <span class="math inline">\(f_{kj}\)</span> is a parameter of binomial distribution, so we naturally to choose the conjugate distribution, beta distribution, as the parameterized variational family of <span class="math inline">\(f_{kj}\)</span>. <span class="math inline">\(p_i=(p_{i1},\ldots,p_{iK})\)</span> is a parameter of multinomial distribution, so we naturally to choose the conjugate distribution, Dirichlet distribution, as the parameterized variational family of <span class="math inline">\(p_i\)</span>. <span class="math inline">\(z_{ij}^a=(z_{ij1}^a,\ldots,z_{ijK}^a)\)</span> obey multinomial distribution, so we naturally to choose multinomial distribution as the parameterized variational family of <span class="math inline">\(z_{ij}^a\)</span>.</p>
<p>  Using  and independence, we choose the variational family as <span class="math display">\[Q(Z,P,F)=\prod_{i=1}^I\prod_{j=1}^J\prod_{a=1}^2Q(z_{ij}^a)\cdot\prod_{i=1}^IQ(p_i)\cdot\prod_{j=1}^J\prod_{k=1}^KQ(f_{kj}),\]</span> where each factor can then be written as <span class="math display">\[Q(z_{ij}^a)\sim Multinomial(\tilde{z_{ij}^a}),\]</span> <span class="math display">\[Q(p_i)\sim Dirichlet(\tilde{p_i}),\]</span> <span class="math display">\[Q(f_{kj})\sim Beta(\tilde{f_{kj}^1},\tilde{f_{kj}^2}).\]</span> <span class="math inline">\(\tilde{z_{ij}^a}\)</span>, <span class="math inline">\(\tilde{p_i}\)</span>, <span class="math inline">\(\tilde{f_{kj}^1}\)</span>, <span class="math inline">\(\tilde{f_{kj}^2}\)</span> are the parameters of the variational distributions (variational parameters).</p>
</div>
<div class="section level3">
<h3 id="elbo">ELBO<a class="anchor" aria-label="anchor" href="#elbo"></a>
</h3>
<p>  We repeat the idea of variational inference. The KL divergence quantifies the tightness of a lower bound to the log-marginal likelihood of the data. Specifically, for any variational distribution <span class="math inline">\(Q(Z,P,F)\)</span>, we have <span class="math display">\[logP(G)=ELBO(Q(Z,P,F)) + KL(Q(Z,P,F)\|P(Z,P,F|G)),\]</span> where we omit the model parameter <span class="math inline">\(K\)</span> (which is not involved in the estimation). Thus, minimizing the KL divergence is equivalent to maximizing the log-marginal likelihood lower bound (ELBO) of the data <span class="math display">\[\begin{split}
&amp;\hat{Q}(Z,P,F)\\
=&amp;\mathop{argmin}\limits_{Q(Z,P,F)}KL(Q(Z,P,F)\|P(Z,P,F|G))\\
=&amp;\mathop{argmin}\limits_{Q(Z,P,F)}\Big[logP(G)-ELBO(Q(Z,P,F))\Big]\\
=&amp;\mathop{argmax}\limits_{Q(Z,P,F)}ELBO(Q(Z,P,F)).
\end{split}\]</span></p>
<p>  The ELBO of the observed genotypes can be written as <span class="math display">\[\begin{split}
&amp;ELBO(Q(Z,P,F))\\
=&amp;
\end{split}\]</span></p>
<p>LDA</p>
</div>
<div class="section level3">
<h3 id="priors">Priors<a class="anchor" aria-label="anchor" href="#priors"></a>
</h3>
<p>different priors</p>
</div>
<div class="section level3">
<h3 id="update-parameters">Update parameters<a class="anchor" aria-label="anchor" href="#update-parameters"></a>
</h3>
<p>Lagrange</p>
</div>
</div>
<div class="section level2">
<h2 id="acceleration">Acceleration<a class="anchor" aria-label="anchor" href="#acceleration"></a>
</h2>
<p>  </p>
</div>
<div class="section level2">
<h2 id="algorithm-implementation">Algorithm Implementation<a class="anchor" aria-label="anchor" href="#algorithm-implementation"></a>
</h2>
<p>  </p>
</div>
<div class="section level2">
<h2 class="unnumbered" id="literature-cited">Literature Cited<a class="anchor" aria-label="anchor" href="#literature-cited"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-blei2017variational" class="csl-entry">
Blei, David M, Alp Kucukelbir, and Jon D McAuliffe. 2017. <span>“Variational Inference: A Review for Statisticians.”</span> <em>Journal of the American Statistical Association</em> 112 (518): 859–77.
</div>
<div id="ref-raj2014faststructure" class="csl-entry">
Raj, Anil, Matthew Stephens, and Jonathan K Pritchard. 2014. <span>“fastSTRUCTURE: Variational Inference of Population Structure in Large SNP Data Sets.”</span> <em>Genetics</em> 197 (2): 573–89.
</div>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Jonathon Chow.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
