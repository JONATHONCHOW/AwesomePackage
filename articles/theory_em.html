<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="AwesomePackage">
<title>Models and Methods I: Fit PSD Model by EM Algorithm • AwesomePackage</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.1.3/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.1.3/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Models and Methods I: Fit PSD Model by EM Algorithm">
<meta property="og:description" content="AwesomePackage">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">AwesomePackage</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.4.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/AwesomePackage.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/application_hgdp.html">Applications III: HGDP Data Set</a>
    <a class="dropdown-item" href="../articles/application_simu.html">Applications I: Simulated Data Sets</a>
    <a class="dropdown-item" href="../articles/application_tgp.html">Applications II: TGP Data Set</a>
    <a class="dropdown-item" href="../articles/theory_em.html">Models and Methods I: Fit PSD Model by EM Algorithm</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/JONATHONCHOW/AwesomePackage/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Models and Methods I: Fit PSD Model by EM Algorithm</h1>
                        <h4 data-toc-skip class="author">Jonathon Chow</h4>
            
            <h4 data-toc-skip class="date">2022-09-27</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/JONATHONCHOW/AwesomePackage/blob/HEAD/vignettes/theory_em.Rmd" class="external-link"><code>vignettes/theory_em.Rmd</code></a></small>
      <div class="d-none name"><code>theory_em.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>  We use the Expectation-Maximization algorithm (EM) to fit the PSD model <span class="citation">(Tang et al. 2005)</span>.</p>
</div>
<div class="section level2">
<h2 id="psd-model">PSD Model<a class="anchor" aria-label="anchor" href="#psd-model"></a>
</h2>
<p>  The typical data set consists of genotypes at a large number <span class="math inline">\(J\)</span> of single nucleotide polymorphisms (SNPs) from a large number <span class="math inline">\(I\)</span> of unrelated individuals. These individuals are drawn from an admixed population with contributions from <span class="math inline">\(K\)</span> postulated ancestral populations. Population <span class="math inline">\(k\)</span> contributes a fraction <span class="math inline">\(p_{ik}\)</span> of individual <span class="math inline">\(i\)</span>’s genome. Note that <span class="math inline">\(\sum_{k=1}^Kp_{ik}=1\)</span>, and <span class="math inline">\(p_{ik}\geq 0\)</span>. Allele 1 at SNP <span class="math inline">\(j\)</span> has frequency <span class="math inline">\(f_{kj}\)</span> in population <span class="math inline">\(k\)</span>. Note that <span class="math inline">\(0\leq f_{kj}\leq 1\)</span>. As a matter of convention, one can choose allele 1 to be the minor allele and the alternative allele 2 to be the major allele. In our model, both the <span class="math inline">\(p_{ik}\)</span> and the <span class="math inline">\(f_{kj}\)</span> are unknown. We are primarily interested in estimating the <span class="math inline">\(p_{ik}\)</span> to control for ancestry in an association study, but our approach also yields estimates of the <span class="math inline">\(f_{kj}\)</span>.</p>
<p>  Let <span class="math inline">\((g_{ij}^1,g_{ij}^2)\)</span> represents the genotype at marker <span class="math inline">\(j\)</span> of person <span class="math inline">\(i\)</span>, where <span class="math inline">\(g_{ij}^a\)</span> represent the observed number of copies of allele 1 at seat <span class="math inline">\(a\)</span>. Thus, <span class="math inline">\((g_{ij}^1,g_{ij}^2)\)</span> equals <span class="math inline">\((1,1)\)</span>, <span class="math inline">\((1,0)\)</span>, <span class="math inline">\((0,1)\)</span>, or <span class="math inline">\((0,0)\)</span> accordingly, as <span class="math inline">\(i\)</span> has genotype 1/1, 1/2, 2/1, or 2/2 at marker <span class="math inline">\(j\)</span>.</p>
<p>  Note that individuals are formed by the random union of gametes. This produces the binomial proportions <span class="math display">\[P(g_{ij}^a=1)=\sum_{k=1}^Kp_{ik}f_{kj},\quad P(g_{ij}^a=0)=\sum_{k=1}^Kp_{ik}(1-f_{kj}),\quad a=1,2.\]</span> Since individuals <span class="math inline">\(i\)</span>, SNPs <span class="math inline">\(j\)</span>, and seats <span class="math inline">\(a\)</span> are considered independent, the log-likelihood of the entire sample is <span class="math display">\[\mathcal{L}(G|P,F)=\sum_{i=1}^I\sum_{j=1}^j\sum_{a=1}^2\bigg\{g_{ij}^alog\Big[\sum_{k=1}^Kp_{ik}f_{kj}\Big]+(1-g_{ij}^a)log\Big[\sum_{k=1}^Kp_{ik}(1-f_{kj})\Big]\bigg\}\]</span> up to an additive constant that does not enter into the maximization problem. Let <span class="math inline">\(g_{ij}=g_{ij}^1+g_{ij}^2\)</span>. So the log-likelihood can also be expressed as <span class="math display">\[\mathcal{L}(G|P,F)=\sum_{i=1}^I\sum_{j=1}^j\bigg\{g_{ij}log\Big[\sum_{k=1}^Kp_{ik}f_{kj}\Big]+(2-g_{ij})log\Big[\sum_{k=1}^Kp_{ik}(1-f_{kj})\Big]\bigg\}.\]</span> The parameter matrices <span class="math inline">\(P=\{p_{ik}\}\)</span> and <span class="math inline">\(F=\{f_{kj}\}\)</span> have dimensions <span class="math inline">\(I\times K\)</span> and <span class="math inline">\(K\times J\)</span>, for a total of <span class="math inline">\(K(I+J)\)</span> parameters.</p>
<p>  Note that the log-likelihood is invariant under permutations of the labels of the ancestral populations. Thus, the log-likelihood has at least <span class="math inline">\(K!\)</span> equivalent global maxima. In practice, this is a minor nuisance and does not affect the convergence of well-behaved algorithms. The constraints <span class="math inline">\(0\leq f_{kj}\leq 1\)</span>, <span class="math inline">\(p_{ik}\geq 0\)</span>, and <span class="math inline">\(\sum_{k=1}^Kp_{ik}=1\)</span> are more significant hindrances to contriving a good optimization algorithm.</p>
</div>
<div class="section level2">
<h2 id="expectation-maximization-algorithm">Expectation-Maximization Algorithm<a class="anchor" aria-label="anchor" href="#expectation-maximization-algorithm"></a>
</h2>
<p>  Our goal is to solve the MLE problem for the observed variable <span class="math inline">\(x\)</span> <span class="math display">\[\theta_{MLE}=\mathop{argmax}\limits_{\theta}log P(x|\theta).\]</span> However, when the probabilistic model contains both the observed variable <span class="math inline">\(x\)</span> and the latent variable <span class="math inline">\(z\)</span>, MLE often cannot find the analytical solution directly. EM algorithm provides a way to solve MLE iteratively.</p>
<p>  The first thing we notice is that <span class="math display">\[log P(x|\theta) = log P(x,z|\theta)-log P(z|x,\theta) = log \frac{P(x,z|\theta)}{Q(z)} - log \frac{P(x|z,\theta)}{Q(z)},\]</span> Where <span class="math inline">\(Q(z)\)</span> is the undetermined distribution.</p>
<p>  We take the expectation of both sides of this equation, then <span class="math display">\[\mathbb{E}_{Q(z)} \Big[LHS\Big] = \int_{z}log P(x|\theta)Q(z)dz = log P(x|\theta)\int_{z}Q(z)dz = log P(x|\theta),\]</span> <span class="math display">\[\mathbb{E}_{Q(z)} \Big[RHS\Big] = \int_{z}log \frac{P(x,z|\theta)}{Q(z)}Q(z)dz - \int_{z}log \frac{P(x|z,\theta)}{Q(z)}Q(z)dz := \mathcal{L}(Q(z),\theta) - KL(Q(z)\|P(z|x,\theta)).\]</span> Hence, we have <span class="math display">\[log P(x|\theta)=\mathcal{L}(Q(z),\theta) - KL(Q(z)\|P(z|x,\theta)).\]</span></p>
<p>  We use the property of KL divergence <span class="math inline">\(KL(Q\|P)\geq 0\)</span>, The equality holds if and only if <span class="math inline">\(Q=P\)</span>. Thus, we have <span class="math display">\[log P(x|\theta)\geq \mathcal{L}(Q(z),\theta),\]</span> Therefore, we also refer to <span class="math inline">\(\mathcal{L}(Q(z),\theta)\)</span> as the evidence lower bound (ELBO).</p>
<p>  Under the condition of the <span class="math inline">\(i\)</span>th iteration, we first fix parameter <span class="math inline">\(\theta^{(i)}\)</span>, then we take <span class="math inline">\(Q(z)=P(z|x,\theta)\)</span>, and have <span class="math inline">\(log P(x|\theta^{(i)})=\mathcal{L}(Q(z),\theta)\)</span>, that is the E-step. Next, we change the parameter <span class="math inline">\(\theta\)</span>, than maximize the ELBO, and get the parameter <span class="math inline">\(\theta^{(i+1)}\)</span> of the <span class="math inline">\((i+1)\)</span>th iteration, that is the M-step. Finally, some simplification can be carried out to obtain the EM algorithm.</p>
<p>  More precisely, <span class="math display">\[\begin{split}
&amp;\theta^{(i+1)}\\
=&amp; \mathop{argmax}\limits_{\theta}\mathcal{L}(Q(z),\theta) \\
=&amp; \mathop{argmax}\limits_{\theta}\int_{z}log \frac{P(x,z|\theta)}{Q(z)}Q(z)dz \\
=&amp; \mathop{argmax}\limits_{\theta}\int_{z}log \frac{P(x,z|\theta)}{P(z|x,\theta^{(i)})}P(z|x,\theta^{(i)})dz \\
=&amp; \mathop{argmax}\limits_{\theta}\int_{z}log P(x,z|\theta)P(z|x,\theta^{(i)})dz - \int_{z}log P(z|x,\theta^{(i)})P(z|x,\theta^{(i)})dz \\
=&amp; \mathop{argmax}\limits_{\theta}\int_{z}log P(x,z|\theta)P(z|x,\theta^{(i)})dz \\
=&amp; \mathbb{E}_{P(z|x,\theta^{(i)})}\Big[log P(x,z|\theta)\Big].
\end{split}\]</span></p>
<p>  In conclusion, at E-step, we compute the expectation <span class="math display">\[\mathbb{E}_{P(z|x,\theta^{(i)})}\Big[log P(x,z|\theta)\Big]=\int_{z}log P(x,z|\theta)P(z|x,\theta^{(i)})dz,\]</span> and at M-step, we compute the maximization and update the parameters <span class="math display">\[\theta^{(i+1)}=\mathop{argmax}\limits_{\theta}\mathbb{E}_{P(z|x,\theta^{(i)})}\Big[log P(x,z|\theta)\Big],\]</span> until the log-likelihood of incomplete data converges. This is the EM algorithm.</p>
</div>
<div class="section level2">
<h2 id="fit-psd-model-by-em-algorithm">Fit PSD Model by EM Algorithm<a class="anchor" aria-label="anchor" href="#fit-psd-model-by-em-algorithm"></a>
</h2>
<p>  We derive the EM algorithm under the PSD model. The observed variable is <span class="math inline">\(G=\{(g_{ij}^1,g_{ij}^2)\}_{I\times J}\)</span>. The model parameters are <span class="math inline">\(P=\{p_{ik}\}_{I\times K}\)</span> and <span class="math inline">\(F=\{f_{kj}\}_{K\times J}\)</span>. The latent variable is <span class="math inline">\(Z=\{z_{ij}^1,z_{ij}^2\}_{I\times J}\)</span>, <span class="math inline">\(z_{ij}^a\)</span> is an element of the set <span class="math inline">\(\{1,\ldots,K\}\)</span>, denotes the population from which the genes of individual <span class="math inline">\(i\)</span> of marker <span class="math inline">\(j\)</span> at position <span class="math inline">\(a\)</span> really come.</p>
<p>  Consider the log-likelihood of complete data <span class="math display">\[\begin{split}
&amp;log P(G,Z|P,F)\\
=&amp;log P(G|Z,P,F) + log P(Z|P,F) \\
=&amp;\sum_{i=1}^I\sum_{j=1}^J\sum_{k=1}^K\sum_{a=1}^2\bigg\{1(z_{ij}^a=k)\Big[g_{ij}^alogf_{kj}+(2-g_{ij}^a)log(1-f_{kj})\Big]+1(z_{ij}^a=k)logp_{ik}\bigg\}.
\end{split}\]</span></p>
<p>  E-step. Using the linear property of expectations, the expectation after the <span class="math inline">\(t\)</span>th iteration is <span class="math display">\[\begin{split}
&amp;\mathbb{E}_{P(Z|G,P^{(t)},F^{(t)})}\Big[log P(G,Z|P,F)\Big]\\=&amp;\sum_{i=1}^I\sum_{j=1}^J\sum_{k=1}^K\sum_{a=1}^2\bigg\{P(z_{ij}^a=k|G,P^{(t)},F^{(t)})\Big[g_{ij}^alogf_{kj}+(2-g_{ij}^a)log(1-f_{kj})+logp_{ik}\Big]\bigg\}.
\end{split}\]</span> Using the Bayesian formula, then <span class="math display">\[\begin{split}
&amp;P(z_{ij}^a=k|G,P^{(t)},F^{(t)})\\
=&amp; P(z_{ij}^a=k|g_{ij}^a,p_{ik}^{(t)},f_{kj}^{(t)})\\
=&amp; \frac{P(g_{ij}^a|z_{ij}^a=k,p_{ik}^{(t)},f_{kj}^{(t)})P(z_{ij}^a=k|p_{ik}^{(t)},f_{kj}^{(t)})}{\sum_{k=1}^KP(g_{ij}^a|z_{ij}^a=k,p_{ik}^{(t)},f_{kj}^{(t)})P(z_{ij}^a=k|p_{ik}^{(t)},f_{kj}^{(t)})}\\
=&amp; \frac{p_{ik}^{(t)}(f_{kj}^{(t)})^{g_{ij}^a}(1-f_{kj}^{(t)})^{(1-g_{ij}^a)}}{\sum_{k=1}^Kp_{ik}^{(t)}(f_{kj}^{(t)})^{g_{ij}^a}(1-f_{kj}^{(t)})^{(1-g_{ij}^a)}}.\end{split}\]</span></p>
<p>  Next, note that <span class="math display">\[p_{ik}^{(t)}(f_{kj}^{(t)})^{g_{ij}^a}(1-f_{kj}^{(t)})^{(1-g_{ij}^a)}=
\left\{
\begin{aligned}
&amp;p_{ik}^{(t)}f_{kj}^{(t)},&amp;\quad &amp;g_{ij}^a=1\\
&amp;p_{ik}^{(t)}(1-f_{kj}^{(t)}),&amp;\quad &amp;g_{ij}^a=0.
\end{aligned}
\right.
\]</span> Thus, we have <span class="math display">\[
P(z_{ij}^a=k|G,P^{(t)},F^{(t)})=
\left\{
\begin{aligned}
&amp;\frac{p_{ik}^{(t)}f_{kj}^{(t)}}{\sum_{k=1}^Kp_{ik}^{(t)}f_{kj}^{(t)}}:=a_{ijk}^{(t)},&amp;\quad &amp;g_{ij}^a=1\\
&amp;\frac{p_{ik}^{(t)}(1-f_{kj}^{(t)})}{\sum_{k=1}^Kp_{ik}^{(t)}(1-f_{kj}^{(t)})}:=b_{ijk}^{(t)},&amp;\quad &amp;g_{ij}^a=0.
\end{aligned}
\right.
\]</span></p>
<p>  M-step. Calculate the parameters for the <span class="math inline">\((t+1)\)</span>th iteration. The problem is transformed into solving an optimization problem <span class="math display">\[\begin{split}
\mathop{max}\limits_{P,F} &amp; \quad \mathbb{E}_{P(Z|G,P^{(t)},F^{(t)})}\Big[log P(G,Z|P,F)\Big]\\
s.t. &amp; \quad \sum_{k=1}^Kp_{ik}=1,\quad i=1,\ldots,I.
\end{split}\]</span> Using the Lagrange multiplier method, we define the  <span class="math inline">\(\mathcal{L}\)</span> as <span class="math display">\[\mathcal{L} = \mathbb{E}_{P(Z|G,P^{(t)},F^{(t)})}\Big[log P(G,Z|P,F)\Big] + \sum_{i=1}^I\tau_i\Big(1-\sum_{k=1}^Kp_{ik}\Big).\]</span> Take the partial derivatives of <span class="math inline">\(p_{ik}\)</span> and <span class="math inline">\(f_{kj}\)</span> and set them equal to zero <span class="math display">\[\frac{1}{p_{ik}}\sum_{j=1}^J\sum_{a=1}^2P(z_{ij}^a=k|G,P^{(t)},F^{(t)})-\tau_i=0,\quad i=1,\ldots,I,\quad k=1,\ldots,K,\]</span> <span class="math display">\[\sum_{i=1}^I\sum_{a=1}^2P(z^a_{ij}=k|G,P^{(t)},F^{(t)})\Big[g_{ij}^a\frac{1}{f_{kj}}+(1-g_{ij}^a)\frac{1}{1-f_{kj}}\Big]=0,\quad j=1,\ldots,J,\quad k=1,\ldots,K.\]</span></p>
<p>  We sum the first equality over k, then <span class="math display">\[\frac{1}{\tau_i}\sum_{k=1}^K\sum_{j=1}^J\sum_{a=1}^2P(z_{ij}^a=k|G,P^{(t)},F^{(t)})=\sum_{k=1}^Kp_{ik}=1,\quad i=1,\ldots,I.\]</span> Thus, the Lagrange multiplier is <span class="math display">\[\tau_i=\sum_{j=1}^J\sum_{a=1}^2\sum_{k=1}^KP(z_{ij}^a=k|G,P^{(t)},F^{(t)})=\sum_{j=1}^J\sum_{a=1}^21=2J,\quad i=1,\ldots,I.\]</span></p>
<p>  Thus, we obtain the parameter update formula for <span class="math inline">\((t+1)\)</span>th iteration <span class="math display">\[\begin{split}
&amp;p_{ik}^{(t+1)}\\
=&amp;\frac{\sum_{j=1}^J\sum_{a=1}^2g_{ij}^aP(z_{ij}^a=k|G,P^{(t)},F^{(t)})+\sum_{j=1}^J\sum_{a=1}^2(1-g_{ij}^a)P(z_{ij}^a=k|G,P^{(t)},F^{(t)})}{2J}\\
=&amp;\frac{\sum_{j=1}^J\sum_{a=1}^2P(z_{ij}^a=k|G,P^{(t)},F^{(t)})}{2J},\quad i=1,\ldots,I,\quad k=1,\ldots,K,
\end{split}\]</span> <span class="math display">\[\begin{split}
&amp;f_{kj}^{(t+1)}\\
=&amp; \frac{\sum_{i=1}^I\sum_{a=1}^2g_{ij}^aP(z_{ij}^a=k|G,P^{(t)},F^{(t)})}{\sum_{i=1}^I\sum_{a=1}^2g_{ij}^aP(z_{ij}^a=k|G,P^{(t)},F^{(t)})+\sum_{i=1}^I\sum_{a=1}^2(1-g_{ij}^a)P(z_{ij}^a=k|G,P^{(t)},F^{(t)})}\\
=&amp;\frac{\sum_{i=1}^I\sum_{a=1}^2g_{ij}^aP(z_{ij}^a=k|G,P^{(t)},F^{(t)})}{\sum_{i=1}^I\sum_{a=1}^2P(z_{ij}^a=k|G,P^{(t)},F^{(t)})},\quad j=1,\ldots,J,\quad k=1,\ldots,K.
\end{split}\]</span></p>
<p>  Finally, Using the expression for <span class="math inline">\(P(z_{ij}^a=k|G,P^{(t)},F^{(t)})\)</span>, we can get <span class="math display">\[\sum_{a=1}^2g_{ij}^aP(z_{ij}^a=k|G,P^{(t)},F^{(t)})=
\left\{
\begin{aligned}
&amp;2a_{ijk}^{(t)},&amp;(g_{ij}^1,g_{ij}^2)=(1,1)\\
&amp;a_{ijk}^{(t)},&amp;(g_{ij}^1,g_{ij}^2)=(1,0)\\
&amp;a_{ijk}^{(t)},&amp;(g_{ij}^1,g_{ij}^2)=(0,1)\\
&amp;0,&amp;(g_{ij}^1,g_{ij}^2)=(0,0)
\end{aligned}
\right.
=\Big(\sum_{a=1}^2g_{ij}^a\Big)a_{ijk}^{(t)}=g_{ij}a_{ijk}^{(t)},\]</span> <span class="math display">\[\sum_{a=1}^2(1-g_{ij}^a)P(z_{ij}^a=k|G,P^{(t)},F^{(t)})=
\left\{
\begin{aligned}
&amp;0,&amp;(g_{ij}^1,g_{ij}^2)=(1,1)\\
&amp;b_{ijk}^{(t)},&amp;(g_{ij}^1,g_{ij}^2)=(1,0)\\
&amp;b_{ijk}^{(t)},&amp;(g_{ij}^1,g_{ij}^2)=(0,1)\\
&amp;2b_{ijk}^{(t)},&amp;(g_{ij}^1,g_{ij}^2)=(0,0)
\end{aligned}
\right.
=\Big[\sum_{a=1}^2(1-g_{ij}^a)\Big]b_{ijk}^{(t)}=(2-g_{ij})b_{ijk}^{(t)}.\]</span> Then the parameter update formula can be written as <span class="math display">\[p_{ik}^{(t+1)}=\frac{\sum_{j=1}^Jg_{ij}a_{ijk}^{(t)}+\sum_{j=1}^J(2-g_{ij})b_{ijk}^{(t)}}{2J},\quad i=1,\ldots,I,\quad k=1,\ldots,K,\]</span> <span class="math display">\[f_{kj}^{(t+1)}\frac{\sum_{i=1}^Ig_{ij}a_{ijk}^{(t)}}{\sum_{i=1}^Ig_{ij}a_{ijk}^{(t)}+\sum_{i=1}^I(2-g_{ij})b_{ijk}^{(t)}},\quad j=1,\ldots,J,\quad k=1,\ldots,K.\]</span></p>
<p>  In conclusion, at E-step, we compute the expectation <span class="math inline">\(a_{ijk}\)</span> and <span class="math inline">\(b_{ijk}\)</span>, and at M-step, we compute the maximization and update the parameters <span class="math inline">\(p_{ik}\)</span> and <span class="math inline">\(f_{kj}\)</span>, until the log-likelihood of incomplete data <span class="math inline">\(\mathcal{L}(G|P,F)\)</span> converges. This is the EM algorithm for PSD model.</p>
</div>
<div class="section level2">
<h2 id="acceleration">Acceleration<a class="anchor" aria-label="anchor" href="#acceleration"></a>
</h2>
<p>  We can speed up the EM algorithm in two ways. The first is the code level. We write the core parameter update part in C++. The second is the algorithm level, we can use SQUAREM to accelerate the algorithm.</p>
</div>
<div class="section level2">
<h2 id="algorithm-implementation">Algorithm Implementation<a class="anchor" aria-label="anchor" href="#algorithm-implementation"></a>
</h2>
<p>  We present the implementation of EM algorithm in R package AwesomePackage. You can fit the PSD model using the EM algorithm using the function <code>psd_fit_em</code>. At the same time, you can use <code>plot_loss</code> to see changes in log-likelihood and <code>plot_structure</code> to plot structure.</p>
<p>  Here is an example.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/JONATHONCHOW/AwesomePackage" class="external-link">AwesomePackage</a></span><span class="op">)</span></span>
<span><span class="va">G</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">1</span>, <span class="fl">0</span>,<span class="fl">2</span>,<span class="fl">1</span>, <span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">1</span>, <span class="fl">0</span>,<span class="fl">1</span>,<span class="fl">0</span>, <span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span>, <span class="fl">3</span>, <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/psd_fit_em.html">psd_fit_em</a></span><span class="op">(</span><span class="va">G</span>, <span class="fl">2</span>, <span class="fl">1e-5</span>, <span class="fl">50</span><span class="op">)</span></span>
<span><span class="va">result</span></span></code></pre></div>
<pre><code><span><span class="co">## $P</span></span>
<span><span class="co">##              [,1]         [,2]</span></span>
<span><span class="co">## [1,] 2.423370e-07 9.999998e-01</span></span>
<span><span class="co">## [2,] 1.000000e+00 6.428104e-10</span></span>
<span><span class="co">## [3,] 3.108029e-01 6.891971e-01</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $F</span></span>
<span><span class="co">##              [,1]         [,2]         [,3]          [,4]          [,5]</span></span>
<span><span class="co">## [1,] 6.092029e-07 1.000000e+00 2.200155e-19  4.135085e-01 4.789181e-167</span></span>
<span><span class="co">## [2,] 2.765476e-01 4.936331e-06 5.737364e-01 6.555430e-176  3.116884e-01</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $Loss</span></span>
<span><span class="co">## [1] -0.7320094 -0.7076503 -0.7074319 -0.7074250</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $Iterations</span></span>
<span><span class="co">## [1] 40</span></span></code></pre>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">L</span> <span class="op">&lt;-</span> <span class="va">result</span><span class="op">$</span><span class="va">Loss</span></span>
<span><span class="fu"><a href="../reference/plot_loss.html">plot_loss</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">L</span><span class="op">)</span>, <span class="st">"em"</span>, <span class="fl">10</span><span class="op">)</span></span></code></pre></div>
<p><img src="theory_em_files/figure-html/unnamed-chunk-1-1.png" width="700"></p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">P</span> <span class="op">&lt;-</span> <span class="va">result</span><span class="op">$</span><span class="va">P</span></span>
<span><span class="fu"><a href="../reference/plot_structure.html">plot_structure</a></span><span class="op">(</span><span class="va">P</span><span class="op">)</span></span></code></pre></div>
<p><img src="theory_em_files/figure-html/unnamed-chunk-1-2.png" width="700"></p>
<p>  See <a href="https://jonathonchow.github.io/AwesomePackage/">AwesomePackage</a> for details.</p>
</div>
<div class="section level2">
<h2 class="unnumbered" id="literature-cited">Literature Cited<a class="anchor" aria-label="anchor" href="#literature-cited"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-tang2005estimation" class="csl-entry">
Tang, Hua, Jie Peng, Pei Wang, and Neil J Risch. 2005. <span>“Estimation of Individual Admixture: Analytical and Study Design Considerations.”</span> <em>Genetic Epidemiology: The Official Publication of the International Genetic Epidemiology Society</em> 28 (4): 289–301.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Jonathon Chow.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
